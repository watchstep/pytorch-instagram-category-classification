{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Instagram Page Screen Shots in 5 Category](https://www.kaggle.com/datasets/bahramjannesarr/instagram-page-screen-shots-in-5-category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Module Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "import random\n",
    "import os\n",
    "%matplotlib inline\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn # neural network\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torchsummary import summary # 깔끔하게 pytorch 출력\n",
    "    import splitfolders\n",
    "except:\n",
    "    !pip install torchsummary\n",
    "    from torchsummary import summary\n",
    "    !pip install split-folders tdqm  # progress bar도 함께 보여주기 위해 tdqm 같이 써줌\n",
    "    import splitfolders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DEVICE Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 1650 with Max-Q Design'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " PyTorch version :  1.12.1 \n",
      " Device :  cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(' PyTorch version : ', torch.__version__, '\\n', 'Device : ', DEVICE)\n",
    "\n",
    "# 난수 지정\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.cuda.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PARAMETER Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "TEST_RATIO = 0.2\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download Data From Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create New token\n",
    "- move kaggle.json to ~\\.kaggle\\kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: kaggle\n",
      "Version: 1.5.12\n",
      "Summary: Kaggle API\n",
      "Home-page: https://github.com/Kaggle/kaggle-api\n",
      "Author: Kaggle\n",
      "Author-email: support@kaggle.com\n",
      "License: Apache 2.0\n",
      "Location: c:\\users\\kim juii\\appdata\\roaming\\python\\python39\\site-packages\n",
      "Requires: tqdm, requests, python-dateutil, python-slugify, urllib3, six, certifi\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "# !pip install kaggle\n",
    "!pip show kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KAGGLE_USERNAME'] = 'kkkkkkkiii' # your-kaggle-username\n",
    "os.environ['KAGGLE_KEY'] = 'adf7a4ef846bfd6a1b21002d7b3d0b5a' # your-kaggle-api-key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.dataset_download_files('bahramjannesarr/instagram-page-screen-shots-in-5-category', path=\"./dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'instagram-page-screen-shots-in-5-category.zip'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = os.getcwd()\n",
    "dataset_dir = os.path.join(root, 'dataset')\n",
    "\n",
    "zipfile_name = os.listdir(dataset_dir)[0]\n",
    "zipfile_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zipfile_path = os.path.join(dataset_dir, zipfile_name)\n",
    "\n",
    "zipfile.ZipFile(zipfile_path).extractall(path='./dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beauty',\n",
       " 'family',\n",
       " 'fashion',\n",
       " 'fitness',\n",
       " 'food',\n",
       " 'instagram-page-screen-shots-in-5-category.zip']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 3770 files [00:04, 809.81 files/s]\n"
     ]
    }
   ],
   "source": [
    "splitfolders.ratio(dataset_dir, output='./split_dataset', seed=1, ratio=(1-TEST_RATIO, TEST_RATIO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_dir = os.path.join(root, 'split_dataset', 'train')\n",
    "test_dataset_dir = os.path.join(root, 'split_dataset', 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root=train_dataset_dir, transform=transforms.ToTensor())\n",
    "\n",
    "# train_dataset_size = int(TEST_RATIO * len(dataset))\n",
    "# test_dataset_size = len(dataset) - train_dataset_size\n",
    "\n",
    "# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_dataset_size, test_dataset_size])\n",
    "\n",
    "means = torch.zeros(3)\n",
    "stds = torch.zeros(3)\n",
    "\n",
    "for img, label in train_dataset:\n",
    "    means += torch.mean(img, dim = (1,2))\n",
    "    stds += torch.std(img, dim = (1,2))\n",
    "\n",
    "means /= len(train_dataset)\n",
    "stds /= len(train_dataset)\n",
    "    \n",
    "print(f'Means: {means}')\n",
    "print(f'STDs: {stds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "trained_means = [0.6395, 0.5844, 0.5506]\n",
    "trained_stds= [0.2851, 0.2930, 0.3085]\n",
    "\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(img_size), # image를 (224, 224) 사이즈로 변경하되, 변경되는 image pixel 값은 random으로 선택\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=trained_means, std=trained_stds) \n",
    "    # ToTensor()형태로 전환된 image에 대해 또 다른 Normalization을 진행 (mean, std 필요한데 R, G, B 순으로 mean, std Normalization 적용)\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=trained_means, std=trained_stds) \n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=dataset_dir, transform=transforms.ToTensor())\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root=train_dataset_dir, transform=train_transforms)\n",
    "test_dataset = datasets.ImageFolder(root=test_dataset_dir, transform=test_transforms)\n",
    "num_of_classes = len(dataset.classes)\n",
    "\n",
    "print('Classes : ', dataset.classes)\n",
    "print('Num of Classes : ', num_of_classes)\n",
    "print('Total Data Size : ', len(dataset))\n",
    "print('Train Data Size : ', len(train_dataset))\n",
    "print('Test Data Size : ', len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "train_iteration = len(train_loader)\n",
    "test_iteration = len(test_loader)\n",
    "print('Train Iteration : ', train_iteration); print('Test Iteration : ', test_iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Check Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (X_train, y_train) in train_loader:\n",
    "  print('X_train : ', X_train.size(), ' type : ', X_train.type())  # 64개의 RGB (channel 3개)로 이뤄진 224*224 이미지 데이터 [BATCH_SIZE, channel, height, width]\n",
    "  print('y_train : ', y_train.size(), ' type : ', y_train.type())\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100, 100))\n",
    "\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(X_train[i], (1, 2, 0))) # [BATCH_SIZE, Channel, Height, Width]를 시각화하기 위해\n",
    "    plt.title('Class : ' + str(y_train[i].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Define Model - ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total data가 3770장으로, 많은 편은 아니기 때문에 Transfer Learning 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://blog.kakaocdn.net/dn/cjoceg/btq0ulXxaaM/dkAS3aQHHg6BU4ReLgxsFk/img.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet-18,34은 Residual Block 사용\n",
    "ResNet-50은 BottleNeck 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/Pseudo-Lab/pytorch-guide/blob/main/img/03-1.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\"https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion = 4 # 블록 내에서 차원을 증가시키는 3번째 conv layer에서의 확장계수\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=64, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        # ResNext나 WideResNet의 경우 사용\n",
    "        width = int(planes * (base_width / 64.)) * groups\n",
    "        \n",
    "        # Bottleneck Block의 구조\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation) # conv2에서 downsample\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        # 1x1 convolution layer\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        # 3x3 convolution layer\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        # 1x1 convolution layer\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        # skip connection\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet50(pretrained=True).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, num_of_classes)\n",
    "model = model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer / Objective Function Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrpssEntropyLoss()\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b50fd22eab5ffa4f834785faf7f3cabde8a7411c842d3beab903f694883cf14"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
